S.No,Question,Answer
,data,"the quantities, characters, or symbols on which operations are performed by a computer, being stored and transmitted in the form of electrical signals and recorded on magnetic, optical, or mechanical recording media."
,Data Science,"Data science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from noisy, structured and unstructured data, and apply knowledge and actionable insights from data across a broad range of application domains. Data science is related to data mining, machine learning and big data."
,Where should I learn Data Science from?,"Kaggle is the most popular learning platform amongst the respondents. However, there is a possibility that their responses may have been biased on account of Kaggle conducting this survey."
,Do I need an extremely powerful computer to do Data Science?,"While this may be the case for industrial level Deep Learning, an ordinary personal computer suffices for most cases"
,"Can you recommend some blogs, podcasts, courses, etc. that I can follow?","KDNuggets Blog, R Bloggers Blog Aggregator and O’Reilly Data Newsletter are the most popular blogs and podcasts among our respondents"
,What skills are most important in Data Science Jobs?,"Python, Statistics and Visualizations are amongst the most important skills that a Data Scientist must possess while on the job. MOOCs and Enterprise Tools are relatively unnecessary in most Data Science Jobs."
,Where can I get datasets from for my personal and coursework projects?,Dataset Aggregators such as Kaggle and Socrata are the most popular platforms for people to find datasets for personal projects. Generic Google Searches and University/Non Profit Research Group Websites figure in at second and third respectively.
,What does a Data Science Job usually involve?,A very common misconception is that data scientists spend the majority of their time building predictive models and feature engineering.
,What factors should I consider while looking for a Data Science Job?,"Learning Opportunity is the biggest factor that our respondents consider while applying for Data Science Jobs. Office, Languages Used, Salary and Management are the other important factors to consider while looking for a job."
,How important is a degree to be successful in Data Science?,"With the plethora of free resources and professional online certifications available online, a question most of us have in our mind is how important it is to get a relevant University degree in order to train ourselves in the field."
,Where do I start learning Data Science?,Respondents started their Data Science journey with Online Courses and Self Learning. University courses came in second.
,What are the most important things to have on a Data Science resume?,"Work Experience in the field is the most important indicator of a person’s expertise in Data Science. Kaggle Competitions results come in second. Interestingly, our respondents place a high value on Online Courses and Certifications although their presence and potency in a resume is still hotly debated."
,Do I need to know the Math behind the ML Algorithms I use?,"From the responses gathered in the survey, it seems like the majority of our respondents have a fairly good understanding of the math behind the algorithms (enough to explain it to a non technical person). Less than a 1000 people (or 15%) knew just enough to run an ML standard library. Since most of the people in our population are employed, it would be good advice to encourage people to learn the math."
, Where should I look for a job?,"A Company’s Website is the most popular place for job seekers to look for a Data Science Jobs. Job Boards are the second most popular place where people look for jobs.Networking is the most powerful tool to get a data science job. Having a very vibrant presence online also helps as it makes you more visible to Internal Recruiters, the second most successful Employer Search Method."
,Data analyst,"An interpreter of data who typically specializes in identifying trends. They’re similar to data scientists, sans the coding experience. One way to think about data analysts is that they’re junior data scientists on their way to becoming full-fledged data scientists."
,Business analyst,"A business analyst will recommend action based on his or her interpretation of data, such as whether or not a business should continue to sell a particular product. Business analysts can use the work of data scientists to communicate the business side of the data to the ultimate decision-makers."
,Data engineer,"Anyone who designs, QAs, and maintains the systems that data scientists employ daily. Whereas a data scientist might be focused on data analysis, a data engineer focuses more on data preparedness."
,Data governance,"The management of the overall quality, integrity, relevance, and security of available data."
,Data set,"Quite simply, a collection of data, particularly one that is specifically structured. They can be small and simple to work with or large and complex. Yelp’s popular data set, for example, includes over 1.2 million business attributes like hours, parking, availability, and ambiance."
,Data mining,"A process that data scientists employ to find usable models and insights in data sets. They use numerous techniques to accomplish this task such as regression, classification, cluster analysis, and outlier analysis."
,Data visualization,"Any attempt to make data more easily digestible by rendering it in a visual context. Data visualization includes charting, graphing, infographing, and can even include cartooning—in generic use cases."
,Data modeling,"Modeling is all about turning data into predictive and actionable information. “Building models that can predict and explain outcomes,” says Daniel Jebaraj, vice president at syncfusion.com, a company that provides enterprise-grade software to companies for such purposes as data integration and big data processing."
,Data wrangling,"The process of formatting or restructuring raw data to suit specific needs or increase its decision-making power (sometimes referred to as data munging). Think in terms of livestock wrangling, if it helps. To wrangle livestock is to herd or move animals to a specific purpose. Rather than livestock, data scientists have, you guessed it, data. To rein in that raw data, whether for legibility or something else, it needs structure."
,Big data,"Put simply, big data is a collective term that describes data that is too large to fit on a single computer. Conventional tools like SQL and Excel are typically unable to handle big data, so new ones have been developed to take their place."
,Algorithm,"A series of repeatable steps, usually expressed mathematically, to accomplish a specific data science task or solve a problem. An important part of a data scientist’s job is his or her ability to recognize an algorithm’s suitability for certain tasks, as it’s impossible to rely on one algorithm as a panacea to all problems."
,Artificial intelligence,"Well-known by its acronym, AI is the apparent ability of machines to act “intelligently” and has become an increasingly popular and useful area of computer science."
,Machine learning,"The computational process wherein a machine “learns” and adjusts its behaviors based on feedback from data. Usually manifesting as an adaptable algorithm, machine learning helps computers predict outcomes without explicit human input."
,Machine learning engineer,"A machine learning engineer isn’t necessarily expected to understand the predictive models and their underlying mathematics the way a data scientist is. A machine learning engineer is, however, expected to master the software tools that make these models usable."
,Deep learning,"A branch of machine learning that attempts to mirror the neurons and neural networks associated with thinking in human beings. It’s the enemy of many a dystopian sci-fi novel where robots become smarter than humans and cause the downfall of mankind. We’re not quite there yet, but recent advances in artificial intelligence employ deep learning technology for speech recognition, translation, and image recognition software."
,Supervised learning,"This is distinctly different from unsupervised learning, which does not rely on human guidance. An example use case for supervised learning might include a data scientist training an algorithm to recognize images of female human beings using correctly labeled images of female human beings and their characteristics."
,Unsupervised learning,"A branch of machine learning where the algorithm does not rely on human input, and is, instead, self-learning. This more closely resembles what some experts call true artificial intelligence."
,Reinforcement learning,"An area of unsupervised machine learning where the machine seeks to maximize reward. The machine, or “agent,” learns through trial and error as well as reward and punishment."
,API,An acronym that stands for application programming interface. APIs provide users with a set of functions used to interact with and deploy the features of a specific application or service.
,Python,"An object-oriented programming language often used in data science because users have developed an extensive array of tools applicable to the field. Python is free to use for commercial or personal projects, and it’s often commended for its learnability for programmers and non-programmers alike."
,R,"An open-source language and environment for statistical computing and analysis. Like Python, R is often used in data science—and knowledge of it is often expected for job applicants. Sometimes considered more difficult to learn than languages like Python, R shines most brightly for its graphical and plotting capabilities and its many data science-driven packages"
,Ruby,"A scripting language that is also popular with data scientists, though not on the same level as Python and R. It does not contain the volume of specialized libraries available in R and Python, and reasons for using it are mostly historical."
,SQL,"An acronym that stands for structured query language, this programming language is designed to interact with databases. Of course, where databases are involved, data scientists aren’t far away. SQL is another must-learn language for data scientists in the making."
,Excel,"One of the most used spreadsheet applications on the market. There’s no way you haven’t come into contact with Excel. It’s used in data science for obvious reasons, but it’s used in practically every professional environment and, at the very least, a familiarity with it is expected in any job you’ll encounter. Excel does great with crunching numbers; visualizing data; reading, importing, and exporting CSV files commonly used in data science; and much more."
,Hadoop,An open-source software framework that allows data scientists to process big data using clusters of hardware running simple programming models. Many herald Hadoop as a solution to big data problems. It allows you to manage much more data than you can on a single computer.
,Pandas,An open-source software library for Python. The library is widely used in the data science community for data manipulation and analysis because it’s free and distributable under the BSD license.
,Decision tree,"A tool of data scientists and related professions to visually lay out decisions and decision making. As the name suggests, the visual model for the decision-making process is a tree. It’s widely used in data mining and machine learning."
,Unstructured data,"Any data that does not fit a predefined data model. Often this data does not fit into the typical row-column structure of a database. Images, emails, videos, audio, and pretty much anything else that might be difficult to “tabify” might constitute examples of unstructured data. "
,hypothesis,"in ML, hypothesis is sometimes used to refer to a particular model or decision boundary from the hypothesis space. E.g., we select a linear decision boundary from the hypothesis space of all possible hyperplanes; more complicated models have a larger hypothesis space."
,factor,"in statistics, factor means categorical variable; factorial experiment means trying all possible combinations of two or more factors."
,sample,"in machine learning, it is standard to say “N samples” to mean the same thing. E.g., “I will draw N samples from the exponential distribution”."
,underfitting,underfitting refers to a model that is too simple to perform well even on the training data.
,overfitting,"overfitting refers to a model that is too specific to the particular training set, potentially because the model is too complex."
,Type I error,"false positive, meaning it was actually negative but you said positive"
,Type II error,"false negative, meaning it was actually positive but you said negative"
,Business Intelligence ,"BI is the process of analyzing and reporting historical data to guide future decision-making. BI helps leaders make better strategic decisions moving forward by determining what happened in the past using data, like sales statistics and operational metrics."
,Data Engineering.,"Data engineers build the infrastructure through which data is gathered, cleaned, stored and prepped for use by data scientists. Good engineers are invaluable, and building a data science team without them is a “cart before the horse” approach."
,Decision Science," Under the umbrella of data science, decision scientists apply math and technology to solve business problems and add in behavioral science and design thinking (a process that aims to better understand the end user)."
,Artificial Intelligence (AI),"AI computer systems can perform tasks that normally require human intelligence. This doesn’t necessarily mean replicating the human mind, but instead involves using human reasoning as a model to provide better services or create better products, such as speech recognition, decision-making and language translation."
,Machine Learning.,"A subset of AI, machine learning refers to the process by which a system learns from inputted data by identifying patterns in that data, and then applying those patterns to new problems or requests. It allows data scientists to teach a computer to carry out tasks, rather than programming it to carry out each task step-by-step. It’s used, for example, to learn a consumer’s preferences and buying patterns to recommend products on Amazon or sift through resumes to identify the highest-potential job candidates based on key words and phrases."
, Supervised Learning,"This is a specific type of machine learning that involves the data scientist acting as a guide to teach the desired conclusion to the algorithm. For instance, the computer learns to identify animals by being trained on a dataset of images that are properly labeled with each species and its characteristics"
,Classification ,"1. Classification is an example of supervised learning in which an algorithm puts a new piece of data under a pre-existing category, based on a set of characteristics for which the category is already known. For example, it can be used to determine if a customer is likely to spend over $20 online, based on their similarity to other customers who have previously spent that amount."
,Cross validation,"Cross validation is a method to validate the stability, or accuracy, of your machine-learning model. Although there are several types of cross validation, the most basic one involves splitting your training set in two and training the algorithm on one subset before applying it the second subset. Because you know what output you should receive, you can assess a model’s validity"
,Clustering,"1. Clustering is classification but without the supervised learning aspect. With clustering, the algorithm receives inputted data and finds similarities in the data itself by grouping data points together that are alike."
,Deep Learning,"A more advanced form of machine learning, deep learning refers to systems with multiple input/output layers, as opposed to shallow systems with one input/output layer. In deep learning, there are several rounds of data input/output required to assist computers to solve complex, real-world problems."
,A/B Testing.,"Generally used in product development, A/B testing is a randomized experiment in which you test two variants to determine the best course of action. For example, Google famously tested various shades of blue to determine which shade earned the most clicks."
,Hypothesis Testing,Hypothesis testing is the use of statistics to determine the probability that a given hypothesis is true. It’s frequently used in clinical research.
,Statistical Power,"Statistical power is the probability of making the correct decision to reject the null hypothesis when the null hypothesis is false. In other words, it’s the likelihood a study will detect an effect when there is an effect to be detected. A high statistical power means a lower likelihood of concluding incorrectly that a variable has no effect."
,Standard Error,Standard error is the measure of the statistical accuracy of an estimate. A larger sample size decreases the standard error.
,Exploratory Data Analysis,"EDA is often the first step when analyzing datasets. With EDA techniques, data scientists can summarize a dataset’s main characteristics and inform the development of more complex models or logical next steps."
,Data Visualization," A key component of data science, data visualizations are the visual representations of text-based information to better detect and recognize patterns, trends and correlations. It helps people understand the significance of data by placing it in a visual context."
,ETL," ETL is a type of data integration that refers to the three steps (extract, transform, load) used to blend data from multiple sources. It’s often deployed to build a data warehouse. An important aspect of this data warehousing is that it consolidates data from multiple sources and transforms it into a common, useful format. For example, ETL normalizes data from multiple business departments and processes to make it standardized and consistent."
,Data Models ,"1. Data Models define how datasets are connected to each other and how they are processed and stored inside a system. Data models show the structure of a database, including the relationships and constraints, which helps data scientists understand how the data can best be stored and manipulated."
,Data Warehouse,A data warehouse is a repository where all the data collected by an organization is stored and used as a guide to make management decisions.
,Algorithm," A mathematical formula or statistical process used to perform an analysis of data. How is Algorithm is related to Big Data? Even though algorithm is a generic term, Big Data analytics made the term contemporary and more popular. (Bonus: Here’s a pickup line on your date, ‘You show me your algorithms and I’ll show you mine. …. Ok, Ok, I’ll stop! No more cheesy jokes)"
,Analytics,"Most likely, your credit card company sent you year-end statement with all your transactions for the entire year. What if you dug into it to see what % you spent on food, clothing, entertainment etc? You are doing ‘analytics’. You are drawing insights from your raw data which can help you make decisions regarding spending for the upcoming year. What if you did the same exercise on tweets or facebook posts made by your friends/network or your own your company. Now we are talking Big Data analytics. It is about making inferences and story-telling with large sets of data.There are 3 or 4 different types of analytics depending on who you talk to."
,Descriptive Analytics," If you just told me that you spent 25% on food, 35% on clothing, 20% on entertainment and the rest on miscellaneous items last year using your credit card, that is descriptive analytics. Of course, you can go into lot more detail as well."
,Predictive Analytics,"If you analyzed your credit card history for the past 5 years and the split is somewhat consistent, you can safely forecast with high probability that next year will be similar to past years. The fine print here is that this is not about ‘predicting the future’ rather ‘forecasting with probabilities’ of what might happen. In Big data predictive analytics, data scientists may use advanced techniques like data mining, machine learning, and advanced statistical processes (we’ll discuss all these terms later) to forecast weather, economy etc."
,Prescriptive Analytics,"Still using the credit card transactions example, you may want to find out which spending to target (i.e. food, entertainment, clothing etc.) to make a huge impact on your overall spending. Prescriptive analytics builds on predictive analytics by including ‘actions’ (i.e. reduce food or clothing or entertainment) and analyzing the resulting outcomes to ‘prescribe’ the best category to target to reduce your overall spend. You can extend this to big data and imagine how executives can make data-driven decisions by looking at the impacts of various actions in front of them."
,Batch processing,"Even though Batch data processing has been around since mainframe days, Batch processing gained additional significance with Big Data given the large datasets that it deals with. Batch data processing is an efficient way of processing high volumes of data where a group of transactions is collected over a period of time. Hadoop, which I’ll describe later, is focused on batch data processing."
,Cassandra," a beautiful name, is a popular open source database management system managed by The Apache Software Foundation. Apache can be credited with many big data technologies and Cassandra was designed to handle large volumes of data across distributed servers."
,Cloud computing,"Well, cloud computing has become ubiquitous so it may not be needed here but I included just for completeness sake. It’s essentially software and/or data hosted and running on remote servers and accessible from anywhere on the internet."
,Cluster computing,"It’s a fancy term for computing using a ‘cluster’ of pooled resources of multiple servers. Getting more technical, we might be talking about nodes, cluster management layer, load balancing, and parallel processing etc."
,Dark Data,"This, in my opinion, is coined to scare the living daylights out of senior management. Basically, this refers to all the data that is gathered and processed by enterprises not used for any meaningful purposes and hence it is ‘dark and may never be analyzed. It could be social network feeds, call center logs, meeting notes and what have you. There are many estimates that anywhere from 60-90% of all enterprise data may be ‘dark data’ but who really knows."
,Data lake,"When I first heard of this, I really thought someone was pulling an April fool’s joke. But it’s a real term! Data lake is a large repository of enterprise-wide data in raw format. While we are here, let’s talk about Data warehouse which is similar in concept that it is also a repository for enterprise-wide data but in a structured format after cleaning and integrating with other sources. Data warehouses are typically used for conventional data (but not exclusively). Supposedly data lake makes it easy to access enterprise-wide data you really need to know what you are looking for and how to process it and make intelligent use of it."
,Data mining,"Data mining is about finding meaningful patterns and deriving insights in large sets of data using sophisticated pattern recognition techniques. It is closely related the term Analytics that we discussed earlier in that you mine the data to get analytics. To derive meaningful patterns, data miners use statistics(yup, good old math), machine learning algorithms, and artificial intelligence."
,Data Scientist,"Talk about a career that is HOT! It is someone who can make sense of big data by extracting raw data (did u say from data lake?), massage it, and come up with insights. Some of the skills required for data scientists are what a superman/woman would have: analytics, statistics, computer science, creativity, story-telling and understand business context. No wonder they are so highly paid."
,Distributed File System,"As big data is too large to store on a single system, Distributed File System is a data storage system meant to store large volumes of data across multiple storage devices and will help decrease the cost and complexity of storing large amounts of data."
,Hadoop,"When people think of big data, they immediately think about Hadoop. Hadoop (with its cute elephant logo) is an open source software framework that consists of what is called a Hadoop Distributed File System (HDFS) and allows for storage, retrieval, and analysis of very large data sets using distributed hardware. If you really want to impress someone, talk about YARN (Yet Another Resource Scheduler) which, as the name says it, is a resource scheduler. I am really impressed by the folks who come up with these names. Apache foundation, which came up with Hadoop, is also responsible for Pig, Hive, and Spark (yup, they are all names of various software pieces). Aren’t you impressed with these names?"
,In-memory computing,"In general, any computing that can be done without accessing I/O is expected to be faster. In-memory computing is a technique to moving the working datasets entirely within a cluster's collective memory and avoid writing intermediate calculations to disk. Apache Spark is is an in-memory computing system and it has huge advantage in speed over I/O bound systems like Hadoop's MapReduce."
,IoT,"The latest buzzword is Internet of Things or IOT. IOT is the interconnection of computing devices in embedded objects (sensors, wearables, cars, fridges etc.) via internet and they enable sending / receiving data. IOT generates huge amounts of data presenting many big data analytics opportunities."
,MapReduce,"MapReduce could be little bit confusing but let me give it a try. MapReduce is a programming model and the best way to understand this is to note that Map and Reduce are two separate items. In this, the programming model first breaks up the big data dataset into pieces (in technical terms into ‘tuples’ but let’s not get too technical here) so it can be distributed across different computers in different locations (i.e. cluster computing described earlier) which is essentially the Map part. Then the model collects the results and ‘reduces’ them into one report. MapReduce’s data processing model goes hand-in-hand with hadoop’s distributed file system."
,NoSQL,It almost sounds like a protest against ‘SQL (Structured Query Language) which is the bread-and-butter for traditional Relational Database Management Systems (RDBMS) but NOSQL actually stands for Not ONLY SQL :-). NoSQL actually refers to database management systems that are designed to handle large volumes of data that does not have a structure or what’s technically called a ‘schema’ (like relational databases have). NoSQL databases are often well-suited for big data systems because of their flexibility and distributed-first architecture needed for large unstructured databases.
,Spark (Apache Spark),"Apache Spark is a fast, in-memory data processing engine to efficiently execute streaming, machine learning or SQL workloads that require fast iterative access to datasets. Spark is generally a lot faster than MapReduce that we discussed earlier."
,Stream processing,"Stream processing is designed to act on real-time and streaming data with “continuous” queries. Combined with streaming analytics i.e. the ability to continuously calculate mathematical or statistical analytics on the fly within the stream, stream processing solutions are designed to handle high volume in real time. "
,Hello,Hello data nerds .How are you?
,Do you know about Data science,Yes Sir/Mam.Anything You ask?
,Data,Raw Facts and figure
,logistic regression,Logistic regression measures the relationship between the dependent variable (our label of what we want to predict) and one or more independent variables (our features) by estimating probability using its underlying logistic function (sigmoid).
,Univariate,Univariate data contains only one variable. The purpose of the univariate analysis is to describe the data and find patterns that exist within it. 
,Bivariate,Bivariate data involves two different variables. The analysis of this type of data deals with causes and relationships and the analysis is done to determine the relationship between the two variables.
,Multivariate,"Multivariate data involves three or more variables, it is categorized under multivariate. It is similar to a bivariate but contains more than one dependent variable."
,Forward Selection,We test one feature at a time and keep adding them until we get a good fit
,Backward Selection, We test all the features and start removing them to see what works better
,Recursive Feature Elimination,Recursively looks through all the different features and how they pair together
,You are given a data set consisting of variables with more than 30 percent missing values. How will you deal with them?,"The following are ways to handle missing data values:    If the data set is large, we can just simply remove the rows with missing data values. It is the quickest way; we use the rest of the data to predict the values.               For smaller data sets, we can substitute missing values with the mean or average of the rest of the data using the pandas' data frame in python. There are different ways to do so, such as df.mean(), df.fillna(mean)."
,dimensionality reductio,The Dimensionality reduction refers to the process of converting a data set with vast dimensions into data with fewer dimensions (fields) to convey similar information concisely.
,Monitor,"Constant monitoring of all models is needed to determine their performance accuracy. When you change something, you want to figure out how your changes are going to affect things. This needs to be monitored to ensure it's doing what it's supposed to do."
,recommender systems,A recommender system predicts what a user would rate a specific product based on their preferences.
,RMSE,Root Mean Square Error. 
,MSE, Mean Square Error.
,How can you select k for k-means? ,We use the elbow method to select k for k-means clustering. The idea of the elbow method is to run k-means clustering on the data set where 'k' is the number of clusters.
,How can outlier values be treated?,"You can drop outliers only if it is a garbage value. If the outliers have extreme values, they can be removed. For example, if all the data points are clustered between zero to 10, but one point lies at 100, then we can remove this point."
,How can time-series data be declared as stationery?,It is stationary when the variance and mean of the series are constant with time
,feature vectors,"A feature vector is an n-dimensional vector of numerical features that represent an object. In machine learning, feature vectors are used to represent numeric or symbolic characteristics (called features) of an object in a mathematical way that's easy to analyze."
,root cause analysis,Root cause analysis was initially developed to analyze industrial accidents but is now widely used in other areas. It is a problem-solving technique used for isolating the root causes of faults or problems. A factor is called a root cause if its deduction from the problem-fault-sequence averts the final undesirable event from recurring.
, cross-validation.,Cross-validation is a model validation technique for evaluating how the outcomes of a statistical analysis will generalize to an independent data set. It is mainly used in backgrounds where the objective is to forecast and one wants to estimate how accurately a model will accomplish in practice. 
,collaborative filterin,"Most recommender systems use this filtering process to find patterns and information by collaborating perspectives, numerous data sources, and several agents."
,What is the law of large numbers?,"It is a theorem that describes the result of performing the same experiment very frequently. This theorem forms the basis of frequency-style thinking. It states that the sample mean, sample variance, and sample standard deviation converge to what they are trying to estimate."
,star schema,"It is a traditional database schema with a central table. Satellite tables map IDs to physical names or descriptions and can be connected to the central fact table using the ID fields; these tables are known as lookup tables and are principally useful in real-time applications, as they save a lot of memory. Sometimes, star schemas involve several layers of summarization to recover information faster."
,confounding variables,These are extraneous variables in a statistical model that correlates directly or inversely with both the dependent and the independent variable. The estimate fails to account for the confounding factor.
,eigenvalue,"Eigenvalues are the directions along which a particular linear transformation acts by flipping, compressing, or stretching."
,eigenvector,"Eigenvectors are for understanding linear transformations. In data analysis, we usually calculate the eigenvectors for a correlation or covariance matrix. "
,selection bias,"Selection bias, in general, is a problematic situation in which error is introduced due to a non-random population sample."
,survivorship bias,Survivorship bias is the logical error of focusing on aspects that support surviving a process and casually overlooking those that did not because of their lack of prominence. This can lead to wrong conclusions in numerous ways.
